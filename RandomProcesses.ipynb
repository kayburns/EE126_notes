{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Time Markov Chains - PageRank\n",
    "\n",
    "If I want to characterize the complete behavior of a DTMC, all I need is the joint PMF. Markov chains are memoryless, so this probability can be expressed as the product of conidtional probabilites.\n",
    "\n",
    "#### Markov Property\n",
    "\n",
    "Given the last state, Markov chains have no memory of the history of the process.\n",
    "\n",
    "$$\\Pr(X_n|X_{n-1}, \\ldots, X_1) = \\Pr(X_n|X_{n - 1})$$\n",
    "\n",
    "*(e.g.) Verifying the Markov property*\n",
    "\n",
    "Does the sequence $\\{Y_1, Y_2, \\ldots \\}$ such that $Y_{n + 1} = (Y_n + X_{n + 1})^{(n + 1)}$ obey the Markov property?\n",
    "\n",
    "This obeys the Markov property. $X$ are mutually independent and our expression only depends on $Y_n$.\n",
    "\n",
    "#### Irreducibility\n",
    "\n",
    "We can reach any state from any other state.\n",
    "\n",
    "#### Periodicity\n",
    "\n",
    "There is no long term pattern to the sequence of visits made to any state.\n",
    "\n",
    "$$d(i) = \\text{g.c.d.}\\{n\\geq 1|P^n(i, i)>0\\}$$\n",
    "\n",
    "If $d(i) = 1$, then the Markov chain is aperiodic.\n",
    "\n",
    "#### Invariant Distribution: Big Theorem for Markov Chains\n",
    "\n",
    "All finite Markov chains have an invariant distribution: $\\pi = \\pi P$. This represents the long term average amount of time spent in each state.\n",
    "\n",
    "1. if MC is finite and irreducible, it has a unique invariant distribution $\\pi^*$\n",
    "\n",
    "2. for finite MC, if MC is aperiodic, then $\\lim_{n\\rightarrow\\infty}\\pi_n = \\pi^*$\n",
    "\n",
    "#### Hitting Time\n",
    "\n",
    "What is the average number of transition to get to state X?\n",
    "\n",
    "$$\\beta(i) = E[T_X|X_0 = A]$$\n",
    "\n",
    "$$\\beta(i) = 1 + \\sum_jP(i, j)\\beta(j)$$\n",
    "\n",
    "#### Reversible Markov Chains\n",
    "\n",
    "In reversible Markov chains, instead of moving forward in time, we move backward: $X^r_{(i)} = X_{(t - i)}$\n",
    "\n",
    "Properties\n",
    "\n",
    "1. a reversed Markov chain is also a Markov chain with the same stationary distribution\n",
    "\n",
    "2. for a time reversed Markov chain to be the same as the forward Markov chain:\n",
    "\n",
    "$$\\pi(j)P_{j, i} = \\pi(i)P_{i, j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Process\n",
    "\n",
    "In a Poisson process, we count the number of arrivals at time $t$, $N_t$. Let $S_i$ be the interarrival time from $t_{i - 1}$ to $t_{i}$. $T_n = \\sum_{i = 1}^{n}S_i$.\n",
    "\n",
    "$$S_n \\sim Exp(\\lambda)$$\n",
    "\n",
    "$$N_t \\sim Pois(\\lambda t)$$\n",
    "\n",
    "Properties\n",
    "\n",
    "1. Poission process is memoryless.\n",
    "\n",
    "2. The probability of k arrivals in an interval of length t, $\\Pr(k, t)$, is the same for all t. By taylor series approximation, we see that for a small interval length $\\tau$:\n",
    "$$\\Pr(k, \\tau) = \\frac{\\lambda\\tau^k}{k!}(e^{-\\lambda\\tau}) = \\frac{\\lambda\\tau^k}{k!} \\sum_{i = 0}^\\infty \\frac{(-\\lambda\\tau)^i}{i!}$$\n",
    "$$\\Pr(0, \\tau) = 1 - \\lambda\\tau + o(\\tau)$$\n",
    "$$\\Pr(1, \\tau) = \\lambda\\tau + o(\\tau)$$\n",
    "$$\\Pr(2, \\tau) = o(\\tau)$$\n",
    "3. The distribution of the number of arrivals of a Poisson process with rate $\\lambda$ in an interval of length $t$ is $Pois(\\lambda t)$.\n",
    "\n",
    "$$\\Pr(N_t = k) = \\Pr(k, t) = (\\lambda t)^k\\frac{e^{-\\lambda t}}{k!}$$\n",
    "\n",
    "#### Properties of the $k^{th}$ Arrival Time\n",
    "\n",
    "- $Y_k = \\sum_{i = 1}^k T_i$\n",
    "- $E[Y_k] = \\sum_{i = 1}^k E[T_i] = \\frac k\\lambda$\n",
    "- $Var(Y_k) = \\sum_{i = 1}^k Var(T_i) = \\frac k{\\lambda^2}$\n",
    "- Erlang PDF of Order K: $f_{Y_k}(y) = \\lambda\\frac{(\\lambda y)^{k-1}e^{-\\lambda y}}{(k - 1)!}$\n",
    "\n",
    "#### Poisson Splitting\n",
    "\n",
    "Consider a $Pois(\\lambda)$ process in which an arrival is successfully received with probability $p$. This is a poisson process with rate $\\lambda p$.\n",
    "\n",
    "*(e.g.) Taxis arrive according to $PP(\\lambda)$ and stop with probability $p$. What is the distribution of the time that you wait?*\n",
    "\n",
    "This is a geometric number of exponential random variables. The probability that the $m^{th}$ taxi picks you up is $p(1 - p)^{m-1}$. The time it takes each taxi to arrive is $Exp(\\lambda)$.\n",
    "\n",
    "Using MGF, we proved that this is $Exp(\\lambda p)$. Now we can infer this from the distribution of the number of arrivals, which is $Pois(\\lambda p)$.\n",
    "\n",
    "#### Poisson Merging\n",
    "\n",
    "Consider two Poisson processes with rates $\\lambda$ and $\\mu$. The merging of these two processes is Poisson with rate $\\lambda + \\mu$. An arrival has probability $\\frac{\\lambda}{\\lambda + \\mu}$ and $\\frac{\\mu}{\\lambda + \\mu}$ of coming from the first and second process, respectively.\n",
    "\n",
    "Recall that the merging of two exponential distributions is exponential with rate $\\lambda_1 + \\lambda_2$.\n",
    "\n",
    "#### Random Incidence Paradox\n",
    "\n",
    "Consider an arrival process with rate $\\lambda$. We observe time $t$.\n",
    "\n",
    "The distribution of the time between the last and next arrival is the sum of $2$ exponential random variables.\n",
    "\n",
    "*(e.g.) Given the $k^{th}$ arrival time, the previous $k - 1$ arrivals are distributed uniformly*\n",
    "\n",
    "Given $S_3 = s$, find the joint distribution of $S_1$ and $S_2$.\n",
    "\n",
    "$$\\Pr(S_1 = s_1, S_2 = s_2|S_3 = s_3) = \\frac{\\Pr(S_1 \\in \\{s_1, s_1 + ds_1\\}, S_2 \\in \\{s_2, s_2 + ds_2\\}, S_3 \\in \\{s, s + ds\\})}{\\Pr(S_3 \\in \\{s, s + ds\\})} = \\frac{\\lambda e^{-\\lambda s_1}\\lambda e^{-\\lambda (s_2 - s_1)} e^{-\\lambda (s - s_2)}}{\\frac{\\lambda^3s^2e^{-\\lambda s}}{(2)!}} = \\frac 2{s^2}$$\n",
    "\n",
    "$S_1$ and $S_2$ are distributed uniformly in the interval from $1$ to $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Time Markov Chains\n",
    "\n",
    "A continuous time Markov chain is a heterogeneous Poisson process. Arrival processes cause transitions to new states. We use a rate matrix, $Q(i, j) = \\lambda_{ij}$, to represent transition rates. We want to use the rate matrix to solve the balance equations (flow into the state should equal flow out), so we enforce the constraint that the rows sum to 0. Therefore:\n",
    "\n",
    "- The amount of time spent in any state is $Exp(\\sum_i \\lambda_i)$.\n",
    "\n",
    "- $q(i)$ is the sum of the rates out of $i$: $q(i) = \\sum_{j \\neq i} Q(i, j)$\n",
    "\n",
    "- The from $i$ to itself should balance the rate outward: $Q(i, i) = -q(i)$\n",
    "\n",
    "- The probability that the next transition goes to $j$: $\\Gamma(i, j) = \\frac{Q(i, j)}{q(i)}$\n",
    "\n",
    "\n",
    "- $\\Pr(X_{t + \\epsilon} = j|X_t = i, X_u, u\\leq t)=\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "          P_j(1, \\tau) = \\epsilon Q(i, j),\\:\\forall j\\neq i\\\\\n",
    "          1 + \\epsilon Q(i, j), j = i\\\\\n",
    "        \\end{array}\n",
    "      \\right.\n",
    "  $\n",
    "\n",
    "#### Reduction to a DTMC\n",
    "\n",
    "Normalize with respect to max rate out of any node.\n",
    "\n",
    "#### Recurrence Properties and the Invariant\n",
    "\n",
    "Any Markov chain can be decomposed into communicating classes (strongly connected components). All of the states in strongly connected components have the same recurrence property.\n",
    "\n",
    "- Null Recurrent\n",
    "\n",
    "$$\\sum_{x\\in S}\\mu(x) = \\infty$$\n",
    "\n",
    "If the MC is null recurrent, it's guaranteed to return to the same state infinitely often, but the expected first passage time to state $x$ is infinite.\n",
    "\n",
    "$p_{x, x} = 1$ and $E_x[N(x)] = \\infty$ but $E_x[T_x] = \\infty$.\n",
    "\n",
    "Irreducibility and recurrence implies the existence of a stationary distribution, $\\mu$: $\\mu P = \\mu$\n",
    "\n",
    "- Positive Recurrent\n",
    "\n",
    "$$\\sum_{x\\in S}\\mu(x) < \\infty$$\n",
    "\n",
    "If the MC is positive recurrent, it's guaranteed to return to the same state inifinitely often and the expected first passage time to the state is finite.\n",
    "\n",
    "$p_{x, x} = 1$ and $E_x[N(x)] = \\infty$ and $E_x[T_x] < \\infty$\n",
    "\n",
    "Irreducibility and positive recurrence implies the existence of a finite stationary distribution to which it will always converge, $\\pi$: $\\pi P = \\pi$.\n",
    "\n",
    "($\\pi$ is $\\mu$ scaled).\n",
    "\n",
    "\n",
    "- Transient\n",
    "\n",
    "If the MC is transient, it's not guaranteed to return to the same state. $E_x[N(x)] < \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
